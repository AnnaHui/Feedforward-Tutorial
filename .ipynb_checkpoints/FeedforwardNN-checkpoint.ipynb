{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error =  0.191371694566\n",
      "Epoches needed to train =  500\n",
      "[[  9.66343547e-72]\n",
      " [  1.00000000e+00]\n",
      " [  1.00000000e+00]\n",
      " [  3.94279565e-91]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        self.weights = {}  # weights\n",
    "        self.bias = {} # bias\n",
    "        self.num_layers = 1  # Set initial number of layer to one (input layer)\n",
    "        self.adjustments_w = {}  # adjustements\n",
    "        self.adjustments_b = {}  # adjustements\n",
    "    \n",
    "    def add_layer(self, shape):\n",
    "        # inital weights in range(-1,1) \n",
    "        self.weights[self.num_layers] = 2 * np.random.random(shape) - 1\n",
    "        # inital bias in range(-1,1)\n",
    "        self.bias[self.num_layers] = 2 * np.random.random(shape[1]) - 1\n",
    "        # inital adjustements i\n",
    "        self.adjustments_w[self.num_layers] = np.zeros(shape)\n",
    "        self.adjustments_b[self.num_layers] = np.zeros(shape[1])\n",
    "        # plus num_layer \n",
    "        self.num_layers += 1\n",
    "    \n",
    "    # sigmoid\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    # derivative of sigmoid  \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def forward_propagate(self,data):       \n",
    "        # Progapagate through network and hold values for use in back-propagation\n",
    "        activation_values = {}\n",
    "        activation_values[1] = data\n",
    "        \n",
    "        for layer in range(2,self.num_layers+1):\n",
    "            # h = data.T (batch_size, input_size) * weight(input_size, output_size) + bias(output_size) \n",
    "            data = np.dot(data.T, self.weights[layer-1])+ self.bias[layer-1].T\n",
    "            # a = f(h)\n",
    "            data = self.sigmoid(data).T\n",
    "            activation_values[layer] = data\n",
    "\n",
    "        return activation_values\n",
    "    \n",
    "    def back_propagate(self, outputs, targets):\n",
    "        deltas_w = {}\n",
    "        deltas_b = {}\n",
    "        # Delta of output Layer ( MSE derivative is 2(output-target))\n",
    "        deltas_w[self.num_layers] = 2 * (outputs[self.num_layers] - targets)\n",
    "        deltas_b[self.num_layers] = 2 * (outputs[self.num_layers] - targets)\n",
    "        \n",
    "        # Delta of hidden Layers\n",
    "        for layer in reversed(range(2, self.num_layers)):  # All layers except input/output\n",
    "            a_val = outputs[layer]\n",
    "            weights = self.weights[layer]\n",
    "            bias = self.bias[layer]\n",
    "            prev_deltas_w = deltas_w[layer+1]\n",
    "            prev_deltas_b = deltas_b[layer+1]\n",
    "            \n",
    "            deltas_w[layer] = np.multiply(np.dot(weights, prev_deltas_w), self.sigmoid_derivative(a_val))\n",
    "            deltas_b[layer] = np.multiply(np.dot(bias, prev_deltas_b), self.sigmoid_derivative(a_val))\n",
    "            \n",
    "        # Caclculate total adjustements based on deltas\n",
    "        for layer in range(1, self.num_layers):\n",
    "          \n",
    "         \n",
    "            self.adjustments_w[layer] += np.dot(deltas_w[layer+1],outputs[layer].T).T\n",
    "        \n",
    "            self.adjustments_b[layer] += np.dot(deltas_b[layer+1],1).reshape(-1)\n",
    "\n",
    "    def gradient_descente(self, batch_size, learning_rate):\n",
    "        # Calculate partial derivative and take a step in that direction\n",
    "        for layer in range(1, self.num_layers):\n",
    "            \n",
    "            partial_w = (1/batch_size) * self.adjustments_w[layer]\n",
    "            partial_b = (1/batch_size) * self.adjustments_b[layer]\n",
    "            \n",
    "\n",
    "            self.weights[layer]+= learning_rate * -partial_w\n",
    "            self.bias[layer]+= learning_rate*1e-3 * -partial_b\n",
    "  \n",
    "    \n",
    "    def loss_func(self, outputs, targets):\n",
    "        # MSE error\n",
    "        return 0.5 * np.mean(np.sum(np.power(outputs - targets, 2), axis=1))\n",
    "        \n",
    "        \n",
    "    def train(self, inputs, targets, num_epochs, learning_rate=1, stop_accuracy=1e-5):\n",
    "        error = []\n",
    "        for iteration in range(num_epochs):\n",
    "            for i in range(len(inputs)):\n",
    "                x = inputs[i]\n",
    "                y = targets[i]\n",
    "                # Pass the training set through our neural network\n",
    "                output = self.forward_propagate(x)\n",
    "                \n",
    "                # Calculate the error\n",
    "                loss = self.loss_func(output[self.num_layers - 1], y)\n",
    "                error.append(loss)\n",
    "\n",
    "                # Calculate Adjustements\n",
    "                self.back_propagate(output, y)\n",
    "   \n",
    "            self.gradient_descente(i, learning_rate)\n",
    "\n",
    "            # Check if accuarcy criterion is satisfied\n",
    "            if np.mean(error[-(i+1):]) < stop_accuracy and iteration > 0:\n",
    "                break\n",
    "                \n",
    "\n",
    "        return(np.asarray(error), iteration+1)\n",
    "    \n",
    "    def predict(self,data):\n",
    "        # pass data through pre-trained network\n",
    "        for layer in range(1,self.num_layers):\n",
    "            # h = data (batch_size, input_size) * weight(input_size, output_size) + bias(output_size) \n",
    "            data = np.dot(data, self.weights[layer]) + self.bias[layer]\n",
    "            # a = f(h)\n",
    "            data = self.sigmoid(data)\n",
    "        return data\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # ----------- XOR Function -----------------\n",
    "\n",
    "    # Create instance of a neural network\n",
    "    nn = NeuralNetwork()\n",
    "\n",
    "    # Add Layers (Input layer is created by default)\n",
    "    nn.add_layer((2, 1))\n",
    "\n",
    "#     nn.add_layer((4, 1))\n",
    "\n",
    "    # XOR function\n",
    "    training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2, 1)\n",
    "    training_labels = np.asarray([[0], [1], [1], [0]])\n",
    "    \n",
    "    error, iteration = nn.train(training_data, training_labels, 500)\n",
    "    print('Error = ', np.mean(error[-4:]))\n",
    "    print('Epoches needed to train = ', iteration)\n",
    "\n",
    "    \n",
    "\n",
    "    training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2)\n",
    "    label = nn.predict(training_data)\n",
    "    print(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]]\n"
     ]
    }
   ],
   "source": [
    "training_data = np.asarray([[0, 0], [0, 1], [1, 0], [1, 1]]).reshape(4, 2, 1)\n",
    "print(training_data[0].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
